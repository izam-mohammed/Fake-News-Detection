{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake News Detection Using RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "import seaborn as sns\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "# tf things\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# to download\n",
    "import gdown\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "os.makedirs(os.path.join(os.getcwd(), \"data\"), exist_ok=True)\n",
    "url = \"https://drive.google.com/uc?id=1eXjwRX-Ds48IbSRKAkcoIrEujKuMj5wN\"\n",
    "output = \"data/data.zip\"\n",
    "gdown.download(url, output, quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upzip data\n",
    "with zipfile.ZipFile(os.path.join(\"data\", \"data.zip\"), \"r\") as f:\n",
    "    f.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading df\n",
    "fake_df = pd.read_csv(\"data/Fake.csv\")\n",
    "real_df = pd.read_csv(\"data/True.csv\")\n",
    "\n",
    "fake_df.shape, real_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any missing values in fake data ??\n",
    "\n",
    "fake_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any missing values in real data ??\n",
    "\n",
    "real_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemme just check the fake df !\n",
    "\n",
    "fake_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what about the real one ?\n",
    "\n",
    "real_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# who are the characters in fake data ?\n",
    "\n",
    "fake_df.subject.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oh, then who is the hero in here ?\n",
    "\n",
    "fake_df.subject.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, what about the real ones ?\n",
    "\n",
    "real_df.subject.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hmm, I think subject and date are laggers, we don't need them\n",
    "\n",
    "real_df.drop([\"date\", \"subject\"], axis=1, inplace=True)\n",
    "fake_df.drop([\"date\", \"subject\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give the data some name to call\n",
    "real_df[\"class\"] = 1\n",
    "fake_df[\"class\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# who is biggest and who is smallest\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar('Fake News', len(fake_df), color='orange')\n",
    "plt.bar('Real News', len(real_df), color='green')\n",
    "plt.title('Distribution of Fake News and Real News', size=15)\n",
    "plt.xlabel('News Type', size=15)\n",
    "plt.ylabel('# of News Articles', size=15)\n",
    "\n",
    "\n",
    "total_len = len(fake_df) + len(real_df)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar('Fake News', len(fake_df) / total_len, color='orange')\n",
    "plt.bar('Real News', len(real_df) / total_len, color='green')\n",
    "plt.title('Distribution of Fake News and Real News', size=15)\n",
    "plt.xlabel('News Type', size=15)\n",
    "plt.ylabel('Proportion of News Articles', size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Difference in news articles:',len(fake_df)-len(real_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining both parties\n",
    "\n",
    "news_df = pd.concat([fake_df, real_df], ignore_index=True, sort=False)\n",
    "news_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the title with the text, it is much easier to process this way.\n",
    "\n",
    "news_df['text'] = news_df['title'] + news_df['text']\n",
    "news_df.drop('title', axis=1, inplace=True)\n",
    "news_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting\n",
    "\n",
    "features = news_df['text']\n",
    "targets = news_df['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.20, random_state=18)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the data\n",
    "\n",
    "def normalize(data):\n",
    "    normalized = []\n",
    "    for i in data:\n",
    "        i = i.lower()\n",
    "        # get rid of urls\n",
    "        i = re.sub('https?://\\S+|www\\.\\S+', '', i)\n",
    "        # get rid of non words and extra spaces\n",
    "        i = re.sub('\\\\W', ' ', i)\n",
    "        i = re.sub('\\n', '', i)\n",
    "        i = re.sub(' +', ' ', i)\n",
    "        i = re.sub('^ ', '', i)\n",
    "        i = re.sub(' $', '', i)\n",
    "        normalized.append(i)\n",
    "    return normalized\n",
    "\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 10000\n",
    "tokenizer = Tokenizer(num_words=max_vocab)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text into vectors \n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train))\n",
    "len(X_train), len(X_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some padding\n",
    "\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=256)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post', maxlen=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building RNN\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(max_vocab, 128),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use early stop, which stops when the validation loss no longer improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,validation_split=0.1, batch_size=30, shuffle=True, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualising the training process\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "epochs = history.epoch\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss', size=20)\n",
    "plt.xlabel('Epochs', size=20)\n",
    "plt.ylabel('Loss', size=20)\n",
    "plt.legend(prop={'size': 20})\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(epochs, acc, 'g', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy', size=20)\n",
    "plt.xlabel('Epochs', size=20)\n",
    "plt.ylabel('Accuracy', size=20)\n",
    "plt.legend(prop={'size': 20})\n",
    "plt.ylim((0.5,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "\n",
    "binary_predictions = []\n",
    "\n",
    "for i in pred:\n",
    "    if i >= 0.5:\n",
    "        binary_predictions.append(1)\n",
    "    else:\n",
    "        binary_predictions.append(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval metrics\n",
    "print('Accuracy on testing set:', accuracy_score(binary_predictions, y_test))\n",
    "print('Precision on testing set:', precision_score(binary_predictions, y_test))\n",
    "print('Recall on testing set:', recall_score(binary_predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confustion matrix\n",
    "\n",
    "matrix = confusion_matrix(binary_predictions, y_test, normalize='all')\n",
    "plt.figure(figsize=(16, 10))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(matrix, annot=True, ax = ax)\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted Labels', size=20)\n",
    "ax.set_ylabel('True Labels', size=20)\n",
    "ax.set_title('Confusion Matrix', size=20) \n",
    "ax.xaxis.set_ticklabels([0,1], size=15)\n",
    "ax.yaxis.set_ticklabels([0,1], size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the weights for visualiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = list(tokenizer.word_index.keys())\n",
    "word_index = word_index[:max_vocab-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "out_v = io.open('fakenews_vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('fakenews_meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for num, word in enumerate(word_index):\n",
    "  vec = weights[num+1] # skip 0, it's padding.\n",
    "  out_m.write(word + \"\\n\")\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
